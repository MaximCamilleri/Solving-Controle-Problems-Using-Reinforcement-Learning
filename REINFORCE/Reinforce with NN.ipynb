{"cells":[{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1675346782265,"user":{"displayName":"Max Matthew Camilleri","userId":"03304612238858656310"},"user_tz":-60},"id":"NgzBrXmI-0pe"},"outputs":[],"source":["import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.distributions.categorical import Categorical\n","from collections import deque\n","import warnings\n","import itertools\n","warnings.filterwarnings(\"ignore\")\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73991,"status":"ok","timestamp":1675346775609,"user":{"displayName":"Max Matthew Camilleri","userId":"03304612238858656310"},"user_tz":-60},"id":"EQL7SXAlSUdz","outputId":"74eeac0b-790c-4959-9c00-18dac6b92291"},"outputs":[],"source":["# !pip install gym[box2d]"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1675346791016,"user":{"displayName":"Max Matthew Camilleri","userId":"03304612238858656310"},"user_tz":-60},"id":"J52YXszzo8CP"},"outputs":[],"source":["gamma = 0.99 # high gamma to not deminish future rewards to much\n","# MAX_EPISODES = 4000"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":271,"status":"ok","timestamp":1675346791541,"user":{"displayName":"Max Matthew Camilleri","userId":"03304612238858656310"},"user_tz":-60},"id":"yMtZFNunOm-k"},"outputs":[],"source":["env = gym.make('LunarLander-v2')\n","# env = gym.make('CartPole-v1')"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1675346791541,"user":{"displayName":"Max Matthew Camilleri","userId":"03304612238858656310"},"user_tz":-60},"id":"8gf3yLhEzbwI"},"outputs":[],"source":["class PolicyNet(nn.Module):\n","    def __init__(self, input_size, hidden_units, output_size):\n","        super(PolicyNet, self).__init__()\n","        self.l1 = nn.Linear(input_size, hidden_units)\n","        self.l2 = nn.Linear(hidden_units, output_size)\n","\n","    def forward(self, x):\n","        x = self.l1(x)\n","        x = F.relu(x)\n","        logits = self.l2(x)\n","        probs = F.softmax(logits, dim=-1)\n","        return probs\n","\n","    def __call__(self, x):\n","        out = self.forward(x)\n","        return out"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":1071,"status":"ok","timestamp":1675346793118,"user":{"displayName":"Max Matthew Camilleri","userId":"03304612238858656310"},"user_tz":-60},"id":"tuYExt36Rqwn"},"outputs":[],"source":["policy = PolicyNet(input_size=env.observation_space.shape[0], hidden_units=32, output_size=4)"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1675346815323,"user":{"displayName":"Max Matthew Camilleri","userId":"03304612238858656310"},"user_tz":-60},"id":"uMGtulsJQ-f0"},"outputs":[],"source":["optimizer = torch.optim.AdamW(policy.parameters(), lr=0.01)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1675346815325,"user":{"displayName":"Max Matthew Camilleri","userId":"03304612238858656310"},"user_tz":-60},"id":"yZ4pcj_gnDm_"},"outputs":[],"source":["def discount_rewards(rewards):\n","    discounted_rewards = np.zeros(len(rewards))\n","    cumulative_rewards = 0\n","    for i in reversed(range(0, len(rewards))):\n","        cumulative_rewards = cumulative_rewards * gamma + rewards[i]\n","        discounted_rewards[i] = cumulative_rewards\n","    return discounted_rewards"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Run 2"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode: 50  Average Return: -196.01112879556578\n","Episode: 100  Average Return: -174.7032256679\n","Episode: 150  Average Return: -178.14168698939324\n","Episode: 200  Average Return: -208.1071649408758\n","Episode: 250  Average Return: -184.43530783587482\n","Episode: 300  Average Return: -198.5906182062771\n","Episode: 350  Average Return: -66.33789416522777\n","Episode: 400  Average Return: -44.2536101481303\n","Episode: 450  Average Return: -36.00914857018242\n","Episode: 500  Average Return: -33.51351951429248\n","Episode: 550  Average Return: -42.528271577275056\n","Episode: 600  Average Return: -36.171622832990835\n","Episode: 650  Average Return: -26.67167775048077\n","Episode: 700  Average Return: -39.67647810095923\n","Episode: 750  Average Return: -13.949491669543162\n","Episode: 800  Average Return: -7.193367231394318\n","Episode: 850  Average Return: 4.786907524169813\n","Episode: 900  Average Return: 16.473253433170285\n","Episode: 950  Average Return: 16.330298387456804\n","Episode: 1000  Average Return: 28.17602358390906\n","Episode: 1050  Average Return: 34.1454685280801\n","Episode: 1100  Average Return: 56.107157658241746\n","Episode: 1150  Average Return: 58.500482876622115\n","Episode: 1200  Average Return: 60.08259427384022\n","Episode: 1250  Average Return: 26.640894939746442\n","Episode: 1300  Average Return: 56.99227642484185\n","Episode: 1350  Average Return: 77.75294396379462\n","Episode: 1400  Average Return: 81.69848014779136\n","Episode: 1450  Average Return: 107.58390132483264\n","Episode: 1500  Average Return: 105.68808755963052\n","Episode: 1550  Average Return: 115.1942093639747\n","Episode: 1600  Average Return: 94.26414140301878\n","Episode: 1650  Average Return: 96.11434104992105\n","Episode: 1700  Average Return: 98.50223367946202\n","Episode: 1750  Average Return: 123.846350264021\n","Episode: 1800  Average Return: 85.27102044829014\n","Episode: 1850  Average Return: 101.9534720650721\n","Episode: 1900  Average Return: 118.15126550407918\n","Episode: 1950  Average Return: 113.89231091314734\n","Episode: 2000  Average Return: 107.06895872702144\n","Episode: 2050  Average Return: 106.9640746872781\n","Episode: 2100  Average Return: 115.14491765207434\n","Episode: 2150  Average Return: 134.7790098354942\n","Episode: 2200  Average Return: 121.04775145688433\n","Episode: 2250  Average Return: 91.16325053063582\n","Episode: 2300  Average Return: 94.09647218472192\n","Episode: 2350  Average Return: 100.33615136986803\n","Episode: 2400  Average Return: 125.74591278090948\n","Episode: 2450  Average Return: 134.13069212819843\n","Episode: 2500  Average Return: 117.81133089365193\n","Episode: 2550  Average Return: 119.58213892188341\n","Episode: 2600  Average Return: 120.31263804431848\n","Episode: 2650  Average Return: 140.57273922113828\n","Episode: 2700  Average Return: 129.32687041735704\n","Episode: 2750  Average Return: 91.28481860118269\n","Episode: 2800  Average Return: 110.89721984062604\n","Episode: 2850  Average Return: 121.73220498581071\n","Episode: 2900  Average Return: 74.4772734280328\n","Episode: 2950  Average Return: 117.9344563113531\n","Episode: 3000  Average Return: 130.29614165135555\n","Episode: 3050  Average Return: 123.43124384793497\n","Episode: 3100  Average Return: 92.44326114267967\n","Episode: 3150  Average Return: 120.17143672773491\n","Episode: 3200  Average Return: 134.1874984745238\n","Episode: 3250  Average Return: 106.98842698557273\n","Episode: 3300  Average Return: 126.56001058169814\n","Episode: 3350  Average Return: 137.12063576494438\n","Episode: 3400  Average Return: 137.4101483031531\n","Episode: 3450  Average Return: 87.04773057049175\n","Episode: 3500  Average Return: -18.76191884283054\n","Episode: 3550  Average Return: 116.45335234213064\n","Episode: 3600  Average Return: 102.33296882749839\n","Episode: 3650  Average Return: 154.06919314524836\n","Episode: 3700  Average Return: 175.25779953500322\n","Episode: 3750  Average Return: 173.9252369006459\n","Episode: 3800  Average Return: 135.94597824524234\n","Episode: 3850  Average Return: 149.20355980881664\n","Episode: 3900  Average Return: 68.11856382992175\n","Episode: 3950  Average Return: 88.28459957792091\n","Episode: 4000  Average Return: 121.16858512869214\n","Episode: 4050  Average Return: 106.63843750619417\n","Episode: 4100  Average Return: 128.2707231376938\n","Episode: 4150  Average Return: 82.2012986075393\n","Episode: 4200  Average Return: -18.130093503221833\n","Episode: 4250  Average Return: 94.8407918355387\n","Episode: 4300  Average Return: 20.880966266237575\n","Episode: 4350  Average Return: 77.36757426400669\n","Episode: 4400  Average Return: 137.4183597200703\n","Episode: 4450  Average Return: 148.50061533518473\n","Episode: 4500  Average Return: 119.0320082767179\n","Episode: 4550  Average Return: 19.171417293928002\n","Episode: 4600  Average Return: 106.75639259809162\n","Episode: 4650  Average Return: 116.95951639878884\n","Episode: 4700  Average Return: 169.71108823036877\n","Solved! Episode: 4740 Average Return: 195.9603307752545\n","Solved! Episode: 4741 Average Return: 198.44308555483704\n","Solved! Episode: 4743 Average Return: 195.03744014737558\n","Episode: 4750  Average Return: 186.04331205297981\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10504\\357729441.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["returns_log = deque(maxlen=50)\n","scores = []\n","\n","for episode in itertools.count():\n","    rewards = []\n","    actions = []\n","    states  = []\n","    done = False\n","    state = env.reset()\n","    while not done:\n","        with torch.no_grad():\n","            probs = policy(torch.tensor(state).unsqueeze(0).float())\n","            sampler = Categorical(probs)\n","            action = sampler.sample()\n","\n","        new_state, reward, done, info = env.step(action.item())\n","\n","        states.append(state)\n","        actions.append(action)\n","        rewards.append(reward)\n","\n","        state = new_state\n","\n","    rewards = np.array(rewards)\n","    R = torch.tensor(discount_rewards(rewards))\n","\n","    states = torch.tensor(states).float()\n","    actions = torch.tensor(actions)\n","\n","    probs = policy(states)\n","    sampler = Categorical(probs)\n","    log_probs = sampler.log_prob(actions)   \n","    loss = -torch.mean(log_probs * R) \n","    \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    returns_log.append(np.sum(rewards))\n","    score = np.mean(returns_log)\n","    scores.append(score)\n","    if episode>0 and episode % 50 == 0:\n","        print(f\"Episode: {episode}  Average Return: {score}\")\n","\n","    if score >= 195:\n","        print(f\"Solved! Episode: {episode} Average Return: {score}\")\n","        break"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'plt' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9424\\271317311.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Episode'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reward'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reward vs Episode'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"]}],"source":["(fig, ax) = plt.subplots(1, 1)\n","ax.set_xlabel('Episode')\n","ax.set_ylabel('Reward')\n","ax.set_title('Reward vs Episode')\n","ax.plot(range(1, len(scores) + 1), scores, color='blue', linestyle='-', linewidth=1, label = \"Reward\")\n","ax.axhline(y = 195, color = 'r', linestyle = '--')\n","ax.grid()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["from colabgymrender.recorder import Recorder\n","env = Recorder(env, './video')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["state = env.reset()\n","done = False\n","while not done:\n","    with torch.no_grad():\n","        probs = policy(torch.tensor(state).unsqueeze(0).float())\n","        sampler = Categorical(probs)\n","        action = sampler.sample()\n","    new_state, reward, done, info = env.step(action.item()) \n","env.play()\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1sqU6SpAKEWl38_kvTdAasz2F7Acsy2LR","timestamp":1658144365585},{"file_id":"1b1gnyCrODe-d91bGuCuF2D2mi7Q-k1Am","timestamp":1652367414969},{"file_id":"1lfwXlCp2ZQpoI7lpS5J423hdDVZzRTJP","timestamp":1651942364460}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"vscode":{"interpreter":{"hash":"ba52e550df5dd7c4d4156f043c0b54db698368195948b3b2935474e916126db8"}}},"nbformat":4,"nbformat_minor":0}
