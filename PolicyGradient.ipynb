{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Landing Using Policy Gradient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\maxma\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\plugins\\hparams\\summary.py:202: The name tf.make_tensor_proto is deprecated. Please use tf.compat.v1.make_tensor_proto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import AdamW\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import optuna\n",
    "from optuna.integration.tensorboard import TensorBoardCallback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(IterableDataset):\n",
    "    def __init__(self, env, policy, steps, gamma):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.steps = steps\n",
    "        self.gamma = gamma\n",
    "        self.obs = env.reset()\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        transitions = []\n",
    "\n",
    "        for step in range(self.steps):\n",
    "            with torch.no_grad():\n",
    "                action = self.policy(torch.as_tensor(self.obs, dtype=torch.float32))\n",
    "            action = action.multinomial(1).cpu().numpy()\n",
    "            next_obs, reward, done, info = self.env.step(action.flatten())\n",
    "            transitions.append((self.obs, action, reward, done))\n",
    "            self.obs = next_obs\n",
    "\n",
    "        obs_b, action_b, reward_b, done_b = map(np.stack, zip(*transitions))\n",
    "\n",
    "        running_return = np.zeros(self.env.num_envs, dtype=np.float32)\n",
    "        return_b = np.zeros_like(reward_b)\n",
    "\n",
    "        for row in range(self.steps-1,-1,-1):\n",
    "            running_return = reward_b[row] + (1-done_b[row]) * self.gamma * running_return\n",
    "            return_b[row] = running_return\n",
    "\n",
    "        num_samples = self.env.num_envs * self.steps\n",
    "        obs_b = obs_b.reshape(num_samples, -1)\n",
    "        action_b = action_b.reshape(num_samples, -1)\n",
    "        return_b = return_b.reshape(num_samples, -1)\n",
    "\n",
    "        return_b = (return_b - np.mean(return_b)) / np.std(return_b + 1e-06)\n",
    "\n",
    "        idx = list(range(num_samples))\n",
    "        random.shuffle(idx)\n",
    "\n",
    "        for i in idx:\n",
    "            yield obs_b[i], action_b[i], return_b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units=64, output_size=2):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, output_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        probs = self.model(x)\n",
    "        return probs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.forward(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient():\n",
    "    def __init__(self, trials = None):\n",
    "        self.num_envs = 5\n",
    "        self.hidden_size = 24\n",
    "        self.gamma = 0.99\n",
    "        self.steps = 1500\n",
    "        self.batch_size = 64\n",
    "        self.epochs = 1000\n",
    "        if trials:\n",
    "            self.optimization(trials)\n",
    "\n",
    "        self.env_name = \"LunarLander-v2\"\n",
    "        self.env = gym.vector.make(self.env_name, num_envs=self.num_envs, asynchronous=False)\n",
    "        self.obs_dim = self.env.single_observation_space.shape[0]\n",
    "        self.n_acts = self.env.single_action_space.n\n",
    "        self.createPolicyNet()\n",
    "        \n",
    "        self.data = Data(self.env, self.logits_net, self.steps, self.gamma) # env, policy, steps, gamma\n",
    "        self.loader = DataLoader(self.data, batch_size=self.batch_size)\n",
    "    \n",
    "    def optimization(self, trials):\n",
    "        self.num_envs = trials.suggest_int(\"num_envs\", 1, 10)\n",
    "        self.hidden_size = trials.suggest_int(\"hidden_size\", 16, 32)\n",
    "        self.gamma = trials.suggest_float(\"gamma\", 0.5, 1)\n",
    "        self.steps = trials.suggest_int(\"steps\", 1000, 2000)\n",
    "        self.batch_size = trials.suggest_int(\"batch_size\", 32, 64)\n",
    "        # self.epochs = trials.suggest_int(\"epochs\", 500, 1500)\n",
    "    \n",
    "    def createPolicyNet(self):\n",
    "        self.logits_net = PolicyNet(self.obs_dim, self.hidden_size, self.n_acts)\n",
    "        self.logits_net.apply(self.initialize_weights)\n",
    "        self.optimizer = AdamW(self.logits_net.parameters(), lr=0.0003)\n",
    "\n",
    "    def initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight.data, 1)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight.data)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    # make function to compute action distribution\n",
    "    def get_policy(self, obs):\n",
    "        probs = self.logits_net(obs)\n",
    "        return Categorical(probs=probs)\n",
    "    \n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(self, obs):\n",
    "        return self.get_policy(obs).sample().item()\n",
    "\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(self, obs, act, weights):\n",
    "        probs = self.logits_net(obs)\n",
    "        log_probs = torch.log(probs + 1e-6)\n",
    "        action_log_prob = log_probs.gather(1, act)\n",
    "        return -(action_log_prob * weights).mean()\n",
    "    \n",
    "    def train_one_epoch(self):\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        for batch in self.loader:\n",
    "            with torch.no_grad():\n",
    "                batch_obs, batch_acts, batch_weights = batch\n",
    "\n",
    "            # take a single policy gradient update step\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss = self.compute_loss(obs=batch_obs, act=batch_acts, weights=batch_weights)\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return 0\n",
    "    \n",
    "    def run_test(self, trajectories):\n",
    "        env2 = gym.make(self.env_name)\n",
    "        scores = []\n",
    "        for trajectory in range(trajectories):\n",
    "            trajectory_return = 0\n",
    "            obs = env2.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "                next_obs, reward, done, _ = env2.step(action)\n",
    "                obs = next_obs\n",
    "                trajectory_return += reward\n",
    "            scores.append(trajectory_return)\n",
    "        del env2\n",
    "        return np.mean(scores)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model = PolicyGradient(trial)\n",
    "\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', \"SGD\"])\n",
    "    optimizer = getattr(optim, optimizer_name)\n",
    "    for epoch in range(model.epochs):\n",
    "        model.train_one_epoch()\n",
    "        score = model.run_test(5)\n",
    "    \n",
    "        trial.report(score, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = TensorBoardCallback(\"logs/\", metric_name=\"score\")\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10, timeout = 600, callbacks=[tensorboard_callback])\n",
    "\n",
    "trial = study.best_trial\n",
    "print(f\"Score: {trial.value}\")\n",
    "print(f\"Best Hyperparameters: {trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    train_one_epoch()\n",
    "    score = run_test(5, logis_net)\n",
    "    if epoch>1 and epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch}  Score: {score}' )\n",
    "    if score >= 195:\n",
    "        print(f'Solved! Epoch: {epoch}  Score: {score}')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba52e550df5dd7c4d4156f043c0b54db698368195948b3b2935474e916126db8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
